{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9700dacd-2ebd-4892-87db-d86753b06e49",
   "metadata": {},
   "source": [
    "                                    TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678715c-e733-4d1b-9487-3c2b03b4bd72",
   "metadata": {},
   "source": [
    "Q1.  Describe the purpose and benefits of pooling in CNN.\n",
    "\n",
    "Pooling, also known as subsampling or downsampling, is a fundamental operation in Convolutional Neural Networks (CNNs) primarily used for dimensionality reduction and feature translation in the context of feature maps generated by convolutional layers. The purpose and benefits of pooling in CNNs can be outlined as follows:\n",
    "\n",
    "1. Dimensionality Reduction: One of the primary purposes of pooling is to reduce the spatial dimensions (width and height) of the feature maps while retaining important information. This reduction helps in controlling the number of parameters and computational complexity of the subsequent layers in the network. As the spatial dimensions decrease, the computational burden decreases, allowing for deeper networks to be trained more efficiently.\n",
    "\n",
    "2. Translation Invariance: Pooling helps in achieving translation invariance, which means that the exact location of a feature in the input image becomes less important. By aggregating information from neighboring regions, pooling layers create features that are more robust to small variations in the position of the features within the input image. This property is particularly useful for tasks such as object recognition, where the position of the object in the image may vary.\n",
    "\n",
    "3. Feature Generalization: Pooling aids in capturing the most important features present in the feature maps by summarizing information from local neighborhoods. By selecting the maximum (Max Pooling) or average (Average Pooling) value within a window, pooling layers emphasize the most active features while suppressing noise and irrelevant details. This process helps in generalizing features and making the network more robust to variations and distortions in the input data.\n",
    "\n",
    "4. Parameter Reduction and Overfitting Mitigation: Pooling layers contribute to reducing the number of parameters in the network, which helps in mitigating overfitting, especially in scenarios with limited training data. By summarizing information and discarding redundant details, pooling reduces the risk of overfitting by preventing the network from memorizing the training data.\n",
    "\n",
    "5. Computational Efficiency: Pooling operations are computationally efficient compared to convolutional operations, as they involve simple operations such as max or average calculations within local regions. This efficiency enables faster training and inference times, making CNNs with pooling layers suitable for real-time applications and deployment on resource-constrained devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42822f03-ba81-489b-9138-4b3f14b67a58",
   "metadata": {},
   "source": [
    "Q2. Explain the differences between min pooling and max pooling in CNN.\n",
    "\n",
    "\n",
    "Max pooling and min pooling are two common types of pooling operations used in Convolutional Neural Networks (CNNs) for downsampling feature maps. While both serve the purpose of reducing the spatial dimensions of the feature maps, they differ in how they aggregate information within local regions. Here are the main differences between min pooling and max pooling:\n",
    "\n",
    "1. Aggregation Function:\n",
    "\n",
    "Max Pooling: In max pooling, the maximum value within each local region (typically defined by a sliding window) is retained and propagated to the output feature map. This means that only the most active feature within each region is preserved, discarding less significant features.\n",
    "\n",
    "Min Pooling: In min pooling, the minimum value within each local region is retained and propagated to the output feature map. Unlike max pooling, min pooling emphasizes the least active feature within each region, which can be useful in certain scenarios, such as edge detection or boundary localization.\n",
    "\n",
    "2. Feature Emphasis:\n",
    "\n",
    "Max Pooling: Max pooling emphasizes the most prominent features within each local region, effectively highlighting the presence of specific patterns or structures. It is particularly useful for capturing the most salient features while discarding noise and irrelevant details.\n",
    "\n",
    "Min Pooling: Min pooling, on the other hand, emphasizes the least active features within each region, which may be useful for tasks where the presence of certain structures or boundaries is of interest. It can help in accentuating subtle features and enhancing the network's sensitivity to specific patterns.\n",
    "\n",
    "3. Applications:\n",
    "\n",
    "Max Pooling: Max pooling is commonly used in CNN architectures for tasks such as image classification, object detection, and feature extraction. Its ability to preserve the most dominant features makes it suitable for tasks where identifying key patterns is crucial.\n",
    "\n",
    "Min Pooling: Min pooling is less commonly used compared to max pooling. It finds applications in scenarios where the emphasis is on detecting boundaries, edges, or regions of low intensity. However, its usage is more specialized and less prevalent in standard CNN architectures.\n",
    "\n",
    "4. Robustness to Noise:\n",
    "\n",
    "Max Pooling: Max pooling tends to be more robust to noise compared to min pooling, as it focuses on the most significant features and suppresses the impact of noisy or less relevant information.\n",
    "Min Pooling: Min pooling may amplify the effects of noise, especially if the least active features correspond to noise or artifacts in the input data. Therefore, careful consideration is needed when using min pooling in noisy environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4dff87-4c9b-4cd1-87fb-851823ad97a7",
   "metadata": {},
   "source": [
    "Q3. Discuss the concept of padding in CNN and its significance.\n",
    "\n",
    "Padding is a technique used in Convolutional Neural Networks (CNNs) to preserve the spatial dimensions of feature maps throughout the convolutional layers. It involves adding additional border pixels (zeros or values derived from the image boundary) around the input image or feature map before applying convolutional operations. Padding affects the size of the output feature maps after convolution and plays a significant role in the design and performance of CNN architectures. Here's a discussion on the concept of padding and its significance:\n",
    "\n",
    "1. Preservation of Spatial Information:\n",
    "\n",
    "Padding helps in maintaining the spatial dimensions of feature maps, especially when using convolutional layers with a stride greater than 1. Without padding, applying convolutional operations repeatedly can progressively reduce the spatial dimensions of the feature maps, potentially leading to loss of spatial information and resolution.\n",
    "\n",
    "2. Centering the Kernel:\n",
    "\n",
    "Padding ensures that the convolutional kernel is centered on each input pixel, allowing for better coverage of the input image. When the kernel is centered, it can extract features from all parts of the input image, which is essential for capturing both global and local patterns effectively.\n",
    "\n",
    "3. Mitigation of Border Effects:\n",
    "\n",
    "Without padding, the convolution operation may not fully cover the borders of the input image, leading to border effects where the output feature map size is smaller than the input size. Padding helps in mitigating these effects by providing additional pixels around the borders, ensuring that convolutional operations can be applied uniformly across the entire input image.\n",
    "\n",
    "4. Control Over Output Size:\n",
    "\n",
    "By adjusting the amount of padding applied, the size of the output feature maps can be controlled. This flexibility allows network designers to precisely manage the spatial dimensions of feature maps at each layer, facilitating the design of CNN architectures tailored to specific tasks and input data characteristics.\n",
    "\n",
    "5. Improved Network Performance:\n",
    "\n",
    "Properly padded convolutional layers can lead to better performance in terms of feature representation and learning. Padding helps in preserving spatial information, which is crucial for tasks such as object localization and segmentation, where precise spatial relationships between features are essential for accurate predictions.\n",
    "\n",
    "Handling Arbitrary Input Sizes:\n",
    "\n",
    "Padding enables CNNs to handle input images of arbitrary sizes. By padding the input images to a consistent size before feeding them into the network, CNNs can process inputs with varying dimensions without requiring modifications to the network architecture.\n",
    "Reduction of Boundary Artifacts:\n",
    "\n",
    "Padding can help in reducing boundary artifacts, such as aliasing or distortion effects, that may occur at the edges of feature maps during convolution. By providing additional pixels around the borders, padding ensures that the convolutional operation is applied more smoothly, minimizing the risk of artifacts in the output feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9807b6-e113-4f00-9107-e1e092cff6fa",
   "metadata": {},
   "source": [
    "                                          OPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d47db6-af09-47e9-bc62-2ede21dabdee",
   "metadata": {},
   "source": [
    "Q1. Provide a brief overview on LeNet-5 architecture.\n",
    "\n",
    "LeNet-5 is a convolutional neural network (CNN) architecture designed by Yann LeCun et al. in 1998, primarily for handwritten digit recognition tasks. It played a significant role in popularizing CNNs and laying the foundation for modern deep learning architectures. Here's a brief overview of its architecture:\n",
    "\n",
    "1. Input Layer: The network takes as input grayscale images of size 32x32 pixels.\n",
    "\n",
    "2. Convolutional Layers: LeNet-5 consists of two convolutional layers:\n",
    "\n",
    "Convolutional layer 1: Applies convolution with 6 filters (5x5) followed by a hyperbolic tangent (tanh) activation function.\n",
    "\n",
    "Subsampling layer 1: Performs down-sampling using average pooling over 2x2 regions with a stride of 2.\n",
    "\n",
    "Convolutional layer 2: Applies convolution with 16 filters (5x5) followed by a tanh activation.\n",
    "\n",
    "Subsampling layer 2: Another down-sampling layer, similar to the first one.\n",
    "\n",
    "\n",
    "3. Fully Connected Layers: After the convolutional and subsampling layers, the feature maps are flattened and connected to fully connected layers:\n",
    "\n",
    "Fully connected layer 1: Consists of 120 neurons with a tanh activation function.\n",
    "Fully connected layer 2: Followed by another fully connected layer with 84 neurons, also using a tanh activation function.\n",
    "\n",
    "4. Output Layer: The final layer is a softmax output layer with 10 units, representing the 10 possible classes (digits 0 through 9).\n",
    "\n",
    "5. Activation Functions: Throughout the network, the hyperbolic tangent (tanh) activation function is used in convolutional and fully connected layers.\n",
    "\n",
    "6. Training: LeNet-5 was trained using gradient-based optimization techniques like stochastic gradient descent (SGD) with momentum.\n",
    "\n",
    "7. Loss Function: The network is typically trained using cross-entropy loss, which is suitable for multi-class classification tasks like digit recognition.\n",
    "\n",
    "8. Other Features:\n",
    "\n",
    "LeNet-5 uses local connectivity and shared weights, reducing the number of parameters compared to fully connected networks.\n",
    "It employs a simple architecture with alternating convolutional and subsampling layers, which helps capture hierarchical features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab955b-99e1-4907-b11b-9c1f67648110",
   "metadata": {},
   "source": [
    " Q2. Describe the key components of LeNet-5 and their respective purposes.\n",
    " \n",
    " LeNet-5, being one of the pioneering convolutional neural network (CNN) architectures, comprises several key components, each serving a specific purpose in the network's design. Here's a breakdown of the key components and their purposes:\n",
    " \n",
    " 1. Convolutional Layers:\n",
    "\n",
    "Purpose: The convolutional layers are responsible for extracting features from the input images by performing convolution operations with learnable filters.\n",
    "\n",
    "Components:\n",
    "\n",
    "Convolution Operation: This operation involves sliding a small filter/kernel over the input image, computing element-wise multiplications, and summing the results to produce feature maps.\n",
    "\n",
    "Learnable Filters: These filters are parameters of the network that are learned during the training process to detect various patterns and features present in the input images.\n",
    "\n",
    "2. Subsampling (Pooling) Layers:\n",
    "\n",
    "Purpose: Subsampling layers reduce the spatial dimensions of the feature maps, effectively downsampling them while retaining the most important information.\n",
    "\n",
    "Components:\n",
    "\n",
    "Pooling Operation: Common pooling operations include max pooling or average pooling, where the maximum or average value within each pooling region is retained, respectively.\n",
    "\n",
    "Downsampling: By reducing the spatial dimensions, subsampling layers help in controlling the number of parameters and computational complexity of the network while enhancing translational invariance.\n",
    "\n",
    "3. Fully Connected Layers:\n",
    "\n",
    "Purpose: Fully connected layers at the end of the network serve for classification by taking the high-level features extracted by convolutional and pooling layers and mapping them to class scores.\n",
    "\n",
    "Components:\n",
    "\n",
    "Neurons: Each neuron in the fully connected layers receives inputs from all the neurons in the previous layer and applies a weighted sum followed by an activation function.\n",
    "\n",
    "Activation Functions: Activation functions like hyperbolic tangent (tanh) or rectified linear unit (ReLU) introduce non-linearity into the network, enabling it to learn complex relationships between features and classes.\n",
    "\n",
    "4. Activation Functions:\n",
    "\n",
    "Purpose: Activation functions introduce non-linearities into the network, allowing it to learn and represent complex mappings between inputs and outputs.\n",
    "\n",
    "Components:\n",
    "Tanh Activation: Tanh activation function was commonly used in LeNet-5 for introducing non-linearity while keeping the output values within a bounded range (-1 to 1).\n",
    "\n",
    "ReLU Activation: While not used in LeNet-5, modern architectures often employ Rectified Linear Unit (ReLU) activation for faster convergence and mitigating the vanishing gradient problem.\n",
    "\n",
    "5. Output Layer:\n",
    "\n",
    "Purpose: The output layer generates the final predictions or class probabilities based on the features extracted and processed through the preceding layers.\n",
    "\n",
    "Components:\n",
    "\n",
    "Softmax Activation: Softmax activation function is typically used in the output layer for multi-class classification tasks, producing a probability distribution over the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb9eb8-df47-40f0-aea0-b8271c7169bf",
   "metadata": {},
   "source": [
    "Q3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.\n",
    "\n",
    "LeNet-5, being one of the pioneering convolutional neural network (CNN) architectures, brought several advantages to image classification tasks. However, it also has its limitations, especially when compared to more modern architectures. Let's discuss the advantages and limitations of LeNet-5:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. Hierarchical Feature Extraction: LeNet-5 employs a hierarchical architecture with alternating convolutional and subsampling layers, allowing it to extract features at multiple levels of abstraction. This hierarchical feature extraction is effective in capturing patterns and structures in images.\n",
    "\n",
    "2. Parameter Sharing: LeNet-5 utilizes parameter sharing in convolutional layers, where the same set of weights is used across different spatial locations. This reduces the number of parameters in the network, making it more computationally efficient and reducing the risk of overfitting, especially in scenarios with limited training data.\n",
    "\n",
    "3. Local Connectivity: By focusing on local connectivity within the receptive fields, LeNet-5 can capture spatial dependencies in the input images more effectively. This local connectivity aids in learning translation-invariant features, making the network robust to variations in object position within the image.\n",
    "\n",
    "4. Simple Architecture: LeNet-5 has a relatively simple architecture compared to modern CNNs, which makes it easier to understand, implement, and train. This simplicity also contributes to faster training times and reduced computational overhead, especially on hardware with limited resources.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "1. Limited Capacity: Due to its simple architecture, LeNet-5 has limited capacity to learn complex features and relationships in large and diverse datasets. This limitation may result in suboptimal performance when applied to more challenging image classification tasks beyond simple digit recognition.\n",
    "\n",
    "2. Small Receptive Fields: The size of the receptive fields in LeNet-5's convolutional layers is relatively small (5x5), which may restrict its ability to capture larger and more complex spatial structures present in high-resolution images or objects with intricate details.\n",
    "\n",
    "3. Limited Non-linearity: LeNet-5 predominantly uses the hyperbolic tangent (tanh) activation function, which has limited non-linearity compared to modern activation functions like ReLU. This may constrain the network's ability to model complex relationships in the data, potentially hindering its performance on certain tasks.\n",
    "\n",
    "4. Pooling Loss: While subsampling layers (pooling) help in reducing spatial dimensions and controlling the number of parameters, they also lead to information loss, potentially discarding useful spatial information. This may impact the network's ability to precisely localize objects or capture fine-grained details.\n",
    "\n",
    "5. Performance on Complex Datasets: LeNet-5 was primarily designed for handwritten digit recognition tasks and may not generalize well to more complex datasets with diverse object categories, varying backgrounds, and occlusions. Its performance may degrade significantly when applied to such datasets without appropriate modifications or enhancements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5de0d-6476-4cb2-aeca-8c96ce5d5e3d",
   "metadata": {},
   "source": [
    "Q4. Implement LeNet-5 using a deep learning framework TensorFlow, and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d534837e-5b41-42ca-9812-d972981b0915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a6dd82-6ee3-443f-ae62-4cf761eda884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `newline'\n",
      "/bin/bash: -c: line 1: `/opt/conda/bin/python -m pip install tensorflow==<version>'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==<version>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5a0d48-6e1f-4f57-88f1-5ce46d03c05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 12:36:38.891334: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 12:36:38.958985: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-29 12:36:38.959043: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-29 12:36:38.960811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-29 12:36:38.970428: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 12:36:38.971198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 12:36:40.208723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce47f57f-bde2-4b15-bc2d-698f4f5b8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fb927aa-5373-423d-be46-5f2074960576",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f911c355-0f28-4c0e-83ce-764632351fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f7270528-180c-487e-b647-301de8b96559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647dbe93-8204-4a75-afb8-9655e1c8aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([ layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "369e6f6a-dcec-4859-a0d4-24fb935f4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c11acf1e-bda9-4e6c-a387-bb9296bafb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 5s 11ms/step - loss: 0.3701 - accuracy: 0.8929 - val_loss: 0.1158 - val_accuracy: 0.9640\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 4s 11ms/step - loss: 0.1057 - accuracy: 0.9663 - val_loss: 0.0887 - val_accuracy: 0.9732\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0732 - accuracy: 0.9766 - val_loss: 0.0781 - val_accuracy: 0.9769\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0595 - accuracy: 0.9816 - val_loss: 0.0583 - val_accuracy: 0.9837\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.0476 - accuracy: 0.9851 - val_loss: 0.0649 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.0414 - accuracy: 0.9870 - val_loss: 0.0542 - val_accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.0534 - val_accuracy: 0.9840\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 4s 9ms/step - loss: 0.0290 - accuracy: 0.9911 - val_loss: 0.0508 - val_accuracy: 0.9860\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 3s 9ms/step - loss: 0.0262 - accuracy: 0.9922 - val_loss: 0.0480 - val_accuracy: 0.9857\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 4s 10ms/step - loss: 0.0231 - accuracy: 0.9922 - val_loss: 0.0423 - val_accuracy: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f90c42d6c50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6434181-feab-4e6f-8717-de5bbca66bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0302 - accuracy: 0.9900\n",
      "Test accuracy: 0.9900000095367432\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7ac78-e755-441f-90f1-ef6a7d667905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ecbd43-4355-4e40-ac88-021a2f46c0a1",
   "metadata": {},
   "source": [
    "                                                                    OPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7552e7-2e39-4474-9f41-0678e61ac91b",
   "metadata": {},
   "source": [
    " Q1. Present an overview of the AlexNet acchitecture. \n",
    " \n",
    " AlexNet is a pioneering convolutional neural network (CNN) architecture designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It was the winning entry in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, significantly advancing the field of computer vision. Here's an overview of the AlexNet architecture:\n",
    " \n",
    "1. Input Layer:\n",
    "\n",
    "AlexNet takes as input RGB images of size 224x224 pixels.\n",
    "\n",
    "2. Convolutional Layers:\n",
    "\n",
    "AlexNet consists of five convolutional layers.\n",
    "The first convolutional layer applies 96 filters (11x11x3) with a stride of 4 and uses the ReLU activation function.\n",
    "Subsequent convolutional layers use smaller filter sizes (5x5) and a stride of 1.\n",
    "\n",
    "3. Max Pooling Layers:\n",
    "\n",
    "After the first, second, and fifth convolutional layers, there are max-pooling layers applied over 3x3 regions with a stride of 2.\n",
    "\n",
    "4. Normalization Layers:\n",
    "\n",
    "Local Response Normalization (LRN) layers are used after the first and second convolutional layers to normalize the responses and enhance generalization.\n",
    "\n",
    "5. Flattening Layer:\n",
    "\n",
    "The output from the last convolutional layer is flattened into a single vector to be fed into the fully connected layers.\n",
    "\n",
    "6. Fully Connected Layers:\n",
    "\n",
    "AlexNet contains three fully connected layers with 4096 neurons each, followed by a dropout layer to prevent overfitting.\n",
    "The ReLU activation function is used in the fully connected layers.\n",
    "\n",
    "7. Output Layer:\n",
    "\n",
    "The final layer is a fully connected softmax output layer with 1000 units, representing the 1000 ImageNet classes.\n",
    "\n",
    "8. Dropout:\n",
    "\n",
    "Dropout regularization is applied before the output layer to mitigate overfitting. It randomly drops neurons during training to reduce co-adaptation of neurons.\n",
    "\n",
    "9. Activation Function:\n",
    "\n",
    "AlexNet predominantly uses the Rectified Linear Unit (ReLU) activation function, which introduces non-linearity and accelerates convergence compared to traditional activation functions like tanh or sigmoid.\n",
    "\n",
    "10. Training:\n",
    "\n",
    "AlexNet was trained using stochastic gradient descent (SGD) with momentum.\n",
    "Data augmentation techniques, such as image translations, horizontal reflections, and altering the intensity of RGB channels, were employed during training to increase the size of the training set and improve generalization.\n",
    "\n",
    "Architecture Advancements:\n",
    "\n",
    "AlexNet introduced several architectural innovations, including the use of deeper networks, ReLU activation functions, dropout regularization, and data augmentation, which significantly improved the performance of CNNs on image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a81d1-0073-47fa-8783-fbef39855a23",
   "metadata": {},
   "source": [
    "Q2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough \n",
    "performance. \n",
    "\n",
    "AlexNet introduced several architectural innovations that played a crucial role in its breakthrough performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. These innovations contributed to the improved performance of convolutional neural networks (CNNs) on image classification tasks. Here are the key architectural innovations introduced in AlexNet:\n",
    "\n",
    "1. Deeper Architecture:\n",
    "\n",
    "AlexNet was one of the first CNN architectures to utilize a relatively deep network with multiple layers.\n",
    "Deeper networks allowed for more abstract and hierarchical feature representations to be learned, capturing increasingly complex patterns in the data.\n",
    "\n",
    "2. ReLU Activation Function:\n",
    "\n",
    "AlexNet replaced traditional activation functions like hyperbolic tangent (tanh) or sigmoid with the Rectified Linear Unit (ReLU) activation function.\n",
    "ReLU activation helped mitigate the vanishing gradient problem, enabling faster convergence during training by allowing gradients to flow more freely through the network.\n",
    "The simplicity and computational efficiency of ReLU made it feasible to train deeper networks more effectively.\n",
    "\n",
    "3. Local Response Normalization (LRN):\n",
    "\n",
    "AlexNet introduced Local Response Normalization (LRN) layers after the first and second convolutional layers.\n",
    "LRN layers helped improve generalization by normalizing the responses within local neighborhoods of the feature maps.\n",
    "LRN layers acted as a form of lateral inhibition, enhancing the contrast between activated neurons and suppressing responses of neighboring neurons.\n",
    "\n",
    "4. Overlapping Max Pooling:\n",
    "\n",
    "AlexNet utilized max-pooling layers with overlapping regions (3x3 with a stride of 2) after convolutional layers.\n",
    "Overlapping pooling reduced spatial resolution while preserving more spatial information compared to non-overlapping pooling.\n",
    "It helped capture richer spatial hierarchies and provided some translation invariance while reducing computational complexity.\n",
    "\n",
    "5. Dropout Regularization:\n",
    "\n",
    "AlexNet incorporated dropout regularization before the fully connected layers to prevent overfitting.\n",
    "Dropout randomly drops a fraction of neurons during training, forcing the network to learn more robust features and reducing co-adaptation of neurons.\n",
    "Dropout regularization helped improve generalization and prevent the model from memorizing noise in the training data.\n",
    "\n",
    "6. Data Augmentation:\n",
    "\n",
    "AlexNet employed data augmentation techniques during training, such as image translations, horizontal reflections, and altering the intensity of RGB channels.\n",
    "Data augmentation increased the effective size of the training dataset, providing the model with more diverse examples and improving its ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e2b89-3bc4-4404-861a-6ca86288202d",
   "metadata": {},
   "source": [
    " Q3. Discuss the Role of convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
    " \n",
    " In AlexNet, convolutional layers, pooling layers, and fully connected layers play distinct yet complementary roles in the network's architecture, contributing to its effectiveness in image classification tasks. Here's a discussion of the roles of each of these components in AlexNet:\n",
    " \n",
    "1. Convolutional Layers:\n",
    "\n",
    "Feature Extraction: The primary role of convolutional layers in AlexNet is to extract hierarchical features from input images.\n",
    "\n",
    "Local Receptive Fields: Convolutional layers employ learnable filters that slide across the input image, capturing local patterns and features. These local receptive fields help the network learn spatial hierarchies of features.\n",
    "\n",
    "Non-linearity: After each convolution operation, a non-linear activation function (ReLU) is applied, introducing non-linearity into the network and enabling it to learn complex mappings between inputs and outputs.\n",
    "\n",
    "Hierarchical Representation: Successive convolutional layers capture increasingly abstract and high-level features by combining information from preceding layers. This hierarchical representation enables the network to learn complex patterns and variations present in the input data.\n",
    "\n",
    "2. Pooling Layers:\n",
    "\n",
    "Spatial Subsampling: Pooling layers reduce the spatial dimensions of feature maps while retaining the most salient information.\n",
    "\n",
    "Translation Invariance: By performing max pooling over small regions, pooling layers provide some degree of translation invariance, making the network robust to variations in object position within the image.\n",
    "\n",
    "Dimensionality Reduction: Pooling layers help control the number of parameters and computational complexity of the network by reducing the spatial dimensions of feature maps, thus aiding in preventing overfitting.\n",
    "\n",
    "Feature Generalization: Pooling layers help generalize features by capturing the most dominant features within local regions of the feature maps, further enhancing the network's ability to generalize to unseen data.\n",
    "\n",
    "3. Fully Connected Layers:\n",
    "\n",
    "High-Level Representation: Fully connected layers at the end of the network process the high-level features extracted by convolutional and pooling layers.\n",
    "\n",
    "Classification: The role of fully connected layers is to map the learned features to class scores or probabilities, enabling the network to make predictions about the input image's class.\n",
    "\n",
    "Global Context: Fully connected layers aggregate information from all neurons in the preceding layer, providing a global context for making predictions.\n",
    "\n",
    "Non-linear Mapping: Like convolutional layers, fully connected layers employ non-linear activation functions (ReLU) to introduce non-linearity and enable the network to learn complex decision boundaries between different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011971a8-8b75-4d7b-bbe5-3a56ba4d75c1",
   "metadata": {},
   "source": [
    "Q4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance \n",
    "on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0809233-6e56-459f-aa7d-22d512beb743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 14:59:23.349231: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 14:59:23.416162: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-29 14:59:23.416265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-29 14:59:23.417994: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-29 14:59:23.429358: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 14:59:23.431681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 14:59:24.606741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf650a-2fb3-43f2-9cfa-649ff9057105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
    "x_test = x_test.reshape((-1, 28, 28, 1)).astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287be5-c00b-4d6b-88b5-406eb9169cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ca6fe-990e-44e3-8bca-c32d12802c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AlexNet architecture\n",
    "model = models.Sequential()\n",
    "([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81273f1e-4d6c-4ced-9195-bb11627f0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6130f63-82ed-488b-83c5-a229a4ae4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=8, batch_size=70, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99059f36-5910-4bf2-9b99-1b9c9818eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd591a96-3b19-4d46-8422-06814d601781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2abfb1-375a-4004-8d42-3189fb36db2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601b09d-1e70-4689-8532-63ab62fd4f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
